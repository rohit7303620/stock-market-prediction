{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cedf9fe6-7d2c-4ab1-b623-b472f8bfe82f",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid decimal literal (1169618631.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[1], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    5UPFX422036GZFWV\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid decimal literal\n"
     ]
    }
   ],
   "source": [
    " 5UPFX422036GZFWV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bbca266d-3c97-4af1-8378-fe759ac9a87b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests) (2023.7.22)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b83deec3-a328-49c0-b847-05eae5fd0cf0",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "get() missing 1 required positional argument: 'url'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m 5UPFX422036GZFWV\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Replace with the actual API URL\u001b[39;00m\n\u001b[0;32m      3\u001b[0m params \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkey1\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue1\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkey2\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue2\u001b[39m\u001b[38;5;124m\"\u001b[39m}  \u001b[38;5;66;03m# Optional parameters\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: get() missing 1 required positional argument: 'url'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "url = \" 5UPFX422036GZFWV\"  # Replace with the actual API URL\n",
    "params = {\"key1\": \"value1\", \"key2\": \"value2\"}  # Optional parameters\n",
    "\n",
    "response = requests.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45251185-40cc-4300-8a61-54ad5a0913ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Meta Data': {'1. Information': 'Daily Prices (open, high, low, close) and Volumes', '2. Symbol': 'AAPL', '3. Last Refreshed': '2024-05-31', '4. Output Size': 'Compact', '5. Time Zone': 'US/Eastern'}, 'Time Series (Daily)': {'2024-05-31': {'1. open': '191.4400', '2. high': '192.5700', '3. low': '189.9100', '4. close': '192.2500', '5. volume': '75158277'}, '2024-05-30': {'1. open': '190.7600', '2. high': '192.1800', '3. low': '190.6300', '4. close': '191.2900', '5. volume': '49947941'}, '2024-05-29': {'1. open': '189.6100', '2. high': '192.2470', '3. low': '189.5100', '4. close': '190.2900', '5. volume': '53068016'}, '2024-05-28': {'1. open': '191.5100', '2. high': '193.0000', '3. low': '189.1000', '4. close': '189.9900', '5. volume': '52280051'}, '2024-05-24': {'1. open': '188.8200', '2. high': '190.5800', '3. low': '188.0404', '4. close': '189.9800', '5. volume': '36326975'}, '2024-05-23': {'1. open': '190.9800', '2. high': '191.0000', '3. low': '186.6250', '4. close': '186.8800', '5. volume': '51005924'}, '2024-05-22': {'1. open': '192.2650', '2. high': '192.8231', '3. low': '190.2700', '4. close': '190.9000', '5. volume': '34648547'}, '2024-05-21': {'1. open': '191.0900', '2. high': '192.7300', '3. low': '190.9201', '4. close': '192.3500', '5. volume': '42309401'}, '2024-05-20': {'1. open': '189.3250', '2. high': '191.9199', '3. low': '189.0100', '4. close': '191.0400', '5. volume': '44361275'}, '2024-05-17': {'1. open': '189.5100', '2. high': '190.8100', '3. low': '189.1800', '4. close': '189.8700', '5. volume': '41282925'}, '2024-05-16': {'1. open': '190.4700', '2. high': '191.0950', '3. low': '189.6601', '4. close': '189.8400', '5. volume': '52845230'}, '2024-05-15': {'1. open': '187.9100', '2. high': '190.6500', '3. low': '187.3700', '4. close': '189.7200', '5. volume': '70399988'}, '2024-05-14': {'1. open': '187.5100', '2. high': '188.3000', '3. low': '186.2900', '4. close': '187.4300', '5. volume': '52393619'}, '2024-05-13': {'1. open': '185.4350', '2. high': '187.1000', '3. low': '184.6200', '4. close': '186.2800', '5. volume': '72044809'}, '2024-05-10': {'1. open': '184.9000', '2. high': '185.0900', '3. low': '182.1300', '4. close': '183.0500', '5. volume': '50759496'}, '2024-05-09': {'1. open': '182.5600', '2. high': '184.6600', '3. low': '182.1100', '4. close': '184.5700', '5. volume': '48982972'}, '2024-05-08': {'1. open': '182.8500', '2. high': '183.0700', '3. low': '181.4500', '4. close': '182.7400', '5. volume': '45057087'}, '2024-05-07': {'1. open': '183.4500', '2. high': '184.9000', '3. low': '181.3200', '4. close': '182.4000', '5. volume': '77305771'}, '2024-05-06': {'1. open': '182.3540', '2. high': '184.2000', '3. low': '180.4200', '4. close': '181.7100', '5. volume': '78569667'}, '2024-05-03': {'1. open': '186.6450', '2. high': '187.0000', '3. low': '182.6600', '4. close': '183.3800', '5. volume': '163224109'}, '2024-05-02': {'1. open': '172.5100', '2. high': '173.4150', '3. low': '170.8900', '4. close': '173.0300', '5. volume': '94214915'}, '2024-05-01': {'1. open': '169.5800', '2. high': '172.7050', '3. low': '169.1100', '4. close': '169.3000', '5. volume': '50383147'}, '2024-04-30': {'1. open': '173.3300', '2. high': '174.9900', '3. low': '170.0000', '4. close': '170.3300', '5. volume': '65934776'}, '2024-04-29': {'1. open': '173.3700', '2. high': '176.0300', '3. low': '173.1000', '4. close': '173.5000', '5. volume': '68169419'}, '2024-04-26': {'1. open': '169.8800', '2. high': '171.3400', '3. low': '169.1800', '4. close': '169.3000', '5. volume': '44838354'}, '2024-04-25': {'1. open': '169.5250', '2. high': '170.6100', '3. low': '168.1511', '4. close': '169.8900', '5. volume': '50558329'}, '2024-04-24': {'1. open': '166.5400', '2. high': '169.3000', '3. low': '166.2100', '4. close': '169.0200', '5. volume': '48251835'}, '2024-04-23': {'1. open': '165.3500', '2. high': '167.0500', '3. low': '164.9200', '4. close': '166.9000', '5. volume': '49537761'}, '2024-04-22': {'1. open': '165.5150', '2. high': '167.2600', '3. low': '164.7700', '4. close': '165.8400', '5. volume': '48116443'}, '2024-04-19': {'1. open': '166.2100', '2. high': '166.4000', '3. low': '164.0750', '4. close': '165.0000', '5. volume': '68149377'}, '2024-04-18': {'1. open': '168.0300', '2. high': '168.6400', '3. low': '166.5500', '4. close': '167.0400', '5. volume': '43122903'}, '2024-04-17': {'1. open': '169.6100', '2. high': '170.6500', '3. low': '168.0000', '4. close': '168.0000', '5. volume': '50901210'}, '2024-04-16': {'1. open': '171.7500', '2. high': '173.7600', '3. low': '168.2700', '4. close': '169.3800', '5. volume': '73711235'}, '2024-04-15': {'1. open': '175.3600', '2. high': '176.6300', '3. low': '172.5000', '4. close': '172.6900', '5. volume': '73531773'}, '2024-04-12': {'1. open': '174.2600', '2. high': '178.3600', '3. low': '174.2100', '4. close': '176.5500', '5. volume': '101670886'}, '2024-04-11': {'1. open': '168.3400', '2. high': '175.4600', '3. low': '168.1600', '4. close': '175.0400', '5. volume': '91070275'}, '2024-04-10': {'1. open': '168.8000', '2. high': '169.0900', '3. low': '167.1100', '4. close': '167.7800', '5. volume': '49709336'}, '2024-04-09': {'1. open': '168.7000', '2. high': '170.0800', '3. low': '168.3500', '4. close': '169.6700', '5. volume': '42231444'}, '2024-04-08': {'1. open': '169.0300', '2. high': '169.2000', '3. low': '168.2400', '4. close': '168.4500', '5. volume': '37216858'}, '2024-04-05': {'1. open': '169.5900', '2. high': '170.3900', '3. low': '168.9500', '4. close': '169.5800', '5. volume': '41975776'}, '2024-04-04': {'1. open': '170.2900', '2. high': '171.9200', '3. low': '168.8200', '4. close': '168.8200', '5. volume': '53355055'}, '2024-04-03': {'1. open': '168.7900', '2. high': '170.6800', '3. low': '168.5800', '4. close': '169.6500', '5. volume': '45571129'}, '2024-04-02': {'1. open': '169.0800', '2. high': '169.3400', '3. low': '168.2302', '4. close': '168.8400', '5. volume': '49013991'}, '2024-04-01': {'1. open': '171.1900', '2. high': '171.2500', '3. low': '169.4750', '4. close': '170.0300', '5. volume': '43772506'}, '2024-03-28': {'1. open': '171.7500', '2. high': '172.2300', '3. low': '170.5100', '4. close': '171.4800', '5. volume': '65672690'}, '2024-03-27': {'1. open': '170.4100', '2. high': '173.6000', '3. low': '170.1100', '4. close': '173.3100', '5. volume': '60273265'}, '2024-03-26': {'1. open': '170.0000', '2. high': '171.4200', '3. low': '169.5800', '4. close': '169.7100', '5. volume': '57388449'}, '2024-03-25': {'1. open': '170.5650', '2. high': '171.9400', '3. low': '169.4500', '4. close': '170.8500', '5. volume': '54288328'}, '2024-03-22': {'1. open': '171.7600', '2. high': '173.0500', '3. low': '170.0600', '4. close': '172.2800', '5. volume': '71160138'}, '2024-03-21': {'1. open': '177.0500', '2. high': '177.4900', '3. low': '170.8400', '4. close': '171.3700', '5. volume': '106181270'}, '2024-03-20': {'1. open': '175.7200', '2. high': '178.6700', '3. low': '175.0900', '4. close': '178.6700', '5. volume': '53423102'}, '2024-03-19': {'1. open': '174.3400', '2. high': '176.6050', '3. low': '173.0300', '4. close': '176.0800', '5. volume': '55215244'}, '2024-03-18': {'1. open': '175.5700', '2. high': '177.7100', '3. low': '173.5200', '4. close': '173.7200', '5. volume': '75604184'}, '2024-03-15': {'1. open': '171.1700', '2. high': '172.6200', '3. low': '170.2850', '4. close': '172.6200', '5. volume': '121752699'}, '2024-03-14': {'1. open': '172.9100', '2. high': '174.3078', '3. low': '172.0500', '4. close': '173.0000', '5. volume': '72571635'}, '2024-03-13': {'1. open': '172.7700', '2. high': '173.1850', '3. low': '170.7600', '4. close': '171.1300', '5. volume': '51948951'}, '2024-03-12': {'1. open': '173.1500', '2. high': '174.0300', '3. low': '171.0100', '4. close': '173.2300', '5. volume': '59544927'}, '2024-03-11': {'1. open': '172.9400', '2. high': '174.3800', '3. low': '172.0500', '4. close': '172.7500', '5. volume': '58929918'}, '2024-03-08': {'1. open': '169.0000', '2. high': '173.7000', '3. low': '168.9400', '4. close': '170.7300', '5. volume': '76267041'}, '2024-03-07': {'1. open': '169.1500', '2. high': '170.7300', '3. low': '168.4900', '4. close': '169.0000', '5. volume': '71765061'}, '2024-03-06': {'1. open': '171.0600', '2. high': '171.2400', '3. low': '168.6800', '4. close': '169.1200', '5. volume': '68587707'}, '2024-03-05': {'1. open': '170.7600', '2. high': '172.0400', '3. low': '169.6200', '4. close': '170.1200', '5. volume': '95132355'}, '2024-03-04': {'1. open': '176.1500', '2. high': '176.9000', '3. low': '173.7900', '4. close': '175.1000', '5. volume': '81510101'}, '2024-03-01': {'1. open': '179.5500', '2. high': '180.5300', '3. low': '177.3800', '4. close': '179.6600', '5. volume': '73563082'}, '2024-02-29': {'1. open': '181.2700', '2. high': '182.5700', '3. low': '179.5300', '4. close': '180.7500', '5. volume': '136682597'}, '2024-02-28': {'1. open': '182.5100', '2. high': '183.1200', '3. low': '180.1300', '4. close': '181.4200', '5. volume': '48953939'}, '2024-02-27': {'1. open': '181.1000', '2. high': '183.9225', '3. low': '179.5600', '4. close': '182.6300', '5. volume': '54318851'}, '2024-02-26': {'1. open': '182.2400', '2. high': '182.7600', '3. low': '180.6500', '4. close': '181.1600', '5. volume': '40867421'}, '2024-02-23': {'1. open': '185.0100', '2. high': '185.0400', '3. low': '182.2300', '4. close': '182.5200', '5. volume': '45119677'}, '2024-02-22': {'1. open': '183.4800', '2. high': '184.9550', '3. low': '182.4600', '4. close': '184.3700', '5. volume': '52292208'}, '2024-02-21': {'1. open': '181.9400', '2. high': '182.8888', '3. low': '180.6600', '4. close': '182.3200', '5. volume': '41529674'}, '2024-02-20': {'1. open': '181.7900', '2. high': '182.4300', '3. low': '180.0000', '4. close': '181.5600', '5. volume': '53665553'}, '2024-02-16': {'1. open': '183.4200', '2. high': '184.8500', '3. low': '181.6650', '4. close': '182.3100', '5. volume': '49752465'}, '2024-02-15': {'1. open': '183.5500', '2. high': '184.4900', '3. low': '181.3500', '4. close': '183.8600', '5. volume': '65434496'}, '2024-02-14': {'1. open': '185.3200', '2. high': '185.5300', '3. low': '182.4400', '4. close': '184.1500', '5. volume': '54630517'}, '2024-02-13': {'1. open': '185.7700', '2. high': '186.2100', '3. low': '183.5128', '4. close': '185.0400', '5. volume': '56529529'}, '2024-02-12': {'1. open': '188.4150', '2. high': '188.6700', '3. low': '186.7900', '4. close': '187.1500', '5. volume': '41781934'}, '2024-02-09': {'1. open': '188.6500', '2. high': '189.9900', '3. low': '188.0000', '4. close': '188.8500', '5. volume': '45155216'}, '2024-02-08': {'1. open': '189.3850', '2. high': '189.5350', '3. low': '187.3500', '4. close': '188.3200', '5. volume': '40962046'}, '2024-02-07': {'1. open': '190.6400', '2. high': '191.0500', '3. low': '188.6100', '4. close': '189.4100', '5. volume': '53438955'}, '2024-02-06': {'1. open': '186.8600', '2. high': '189.3100', '3. low': '186.7695', '4. close': '189.3000', '5. volume': '43490759'}, '2024-02-05': {'1. open': '188.1500', '2. high': '189.2500', '3. low': '185.8400', '4. close': '187.6800', '5. volume': '69668820'}, '2024-02-02': {'1. open': '179.8600', '2. high': '187.3300', '3. low': '179.2500', '4. close': '185.8500', '5. volume': '102551680'}, '2024-02-01': {'1. open': '183.9850', '2. high': '186.9500', '3. low': '183.8200', '4. close': '186.8600', '5. volume': '64885408'}, '2024-01-31': {'1. open': '187.0400', '2. high': '187.0950', '3. low': '184.3500', '4. close': '184.4000', '5. volume': '55467803'}, '2024-01-30': {'1. open': '190.9400', '2. high': '191.8000', '3. low': '187.4700', '4. close': '188.0400', '5. volume': '55859370'}, '2024-01-29': {'1. open': '192.0100', '2. high': '192.2000', '3. low': '189.5800', '4. close': '191.7300', '5. volume': '47145622'}, '2024-01-26': {'1. open': '194.2700', '2. high': '194.7600', '3. low': '191.9400', '4. close': '192.4200', '5. volume': '44594011'}, '2024-01-25': {'1. open': '195.2200', '2. high': '196.2675', '3. low': '193.1125', '4. close': '194.1700', '5. volume': '54822126'}, '2024-01-24': {'1. open': '195.4200', '2. high': '196.3800', '3. low': '194.3400', '4. close': '194.5000', '5. volume': '53463269'}, '2024-01-23': {'1. open': '195.0200', '2. high': '195.7500', '3. low': '193.8299', '4. close': '195.1800', '5. volume': '42355590'}, '2024-01-22': {'1. open': '192.3000', '2. high': '195.3300', '3. low': '192.2600', '4. close': '193.8900', '5. volume': '60133852'}, '2024-01-19': {'1. open': '189.3300', '2. high': '191.9500', '3. low': '188.8200', '4. close': '191.5600', '5. volume': '68902985'}, '2024-01-18': {'1. open': '186.0900', '2. high': '189.1400', '3. low': '185.8300', '4. close': '188.6300', '5. volume': '78005754'}, '2024-01-17': {'1. open': '181.2700', '2. high': '182.9300', '3. low': '180.3000', '4. close': '182.6800', '5. volume': '47317433'}, '2024-01-16': {'1. open': '182.1600', '2. high': '184.2600', '3. low': '180.9340', '4. close': '183.6300', '5. volume': '65603041'}, '2024-01-12': {'1. open': '186.0600', '2. high': '186.7400', '3. low': '185.1900', '4. close': '185.9200', '5. volume': '40477782'}, '2024-01-11': {'1. open': '186.5400', '2. high': '187.0500', '3. low': '183.6200', '4. close': '185.5900', '5. volume': '49128408'}, '2024-01-10': {'1. open': '184.3500', '2. high': '186.4000', '3. low': '183.9200', '4. close': '186.1900', '5. volume': '46792908'}, '2024-01-09': {'1. open': '183.9200', '2. high': '185.1500', '3. low': '182.7300', '4. close': '185.1400', '5. volume': '42841809'}}}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# Replace with the actual API URL (including endpoint)\n",
    "url = \"https://www.alphavantage.co/query\"\n",
    "\n",
    "# Replace with your actual API key\n",
    "api_key = \"1EB6SE6BZ0SWH6PE\"\n",
    "\n",
    "# Parameters for the API call\n",
    "params = {\n",
    "    \"function\": \"TIME_SERIES_DAILY\",\n",
    "    \"symbol\": \"AAPL\",\n",
    "    \"apikey\": api_key\n",
    "}\n",
    "\n",
    "# Making the request\n",
    "response = requests.get(url, params=params)\n",
    "\n",
    "# Checking if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Parsing the JSON response\n",
    "    data = response.json()\n",
    "    print(data)\n",
    "else:\n",
    "    print(f\"Error: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6037e546-7cf0-47e2-ab9e-6ab62cb71d4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\AppData\\Local\\Temp\\ipykernel_16140\\4220489524.py:2: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                Open      High       Low     Close    Volume\n",
      "2024-01-09  183.9200  185.1500  182.7300  185.1400  42841809\n",
      "2024-01-10  184.3500  186.4000  183.9200  186.1900  46792908\n",
      "2024-01-11  186.5400  187.0500  183.6200  185.5900  49128408\n",
      "2024-01-12  186.0600  186.7400  185.1900  185.9200  40477782\n",
      "2024-01-16  182.1600  184.2600  180.9340  183.6300  65603041\n",
      "...              ...       ...       ...       ...       ...\n",
      "2024-05-24  188.8200  190.5800  188.0404  189.9800  36326975\n",
      "2024-05-28  191.5100  193.0000  189.1000  189.9900  52280051\n",
      "2024-05-29  189.6100  192.2470  189.5100  190.2900  53068016\n",
      "2024-05-30  190.7600  192.1800  190.6300  191.2900  49947941\n",
      "2024-05-31  191.4400  192.5700  189.9100  192.2500  75158277\n",
      "\n",
      "[100 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# Replace with the actual API URL (including endpoint)\n",
    "url = \"https://www.alphavantage.co/query\"\n",
    "\n",
    "# Replace with your actual API key\n",
    "api_key = \"1EB6SE6BZ0SWH6PE\"\n",
    "\n",
    "# Parameters for the API call\n",
    "params = {\n",
    "    \"function\": \"TIME_SERIES_DAILY\",\n",
    "    \"symbol\": \"AAPL\",\n",
    "    \"apikey\": api_key\n",
    "}\n",
    "\n",
    "# Making the request\n",
    "response = requests.get(url, params=params)\n",
    "\n",
    "# Checking if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Parsing the JSON response\n",
    "    data = response.json()\n",
    "    \n",
    "    # Extracting the time series data\n",
    "    time_series = data.get(\"Time Series (Daily)\", {})\n",
    "    \n",
    "    # Converting the time series data to a DataFrame\n",
    "    df = pd.DataFrame.from_dict(time_series, orient='index')\n",
    "    \n",
    "    # Converting index to datetime\n",
    "    df.index = pd.to_datetime(df.index)\n",
    "    \n",
    "    # Sorting the DataFrame by index (date)\n",
    "    df.sort_index(inplace=True)\n",
    "    \n",
    "    # Renaming columns for better readability\n",
    "    df.columns = ['Open', 'High', 'Low', 'Close', 'Volume']\n",
    "    \n",
    "    # Displaying the DataFrame\n",
    "    print(df)\n",
    "else:\n",
    "    print(f\"Error: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ff47a06-58e9-4e7c-988f-cf089a21d059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                Open      High       Low     Close    Volume\n",
      "2024-01-09  183.9200  185.1500  182.7300  185.1400  42841809\n",
      "2024-01-10  184.3500  186.4000  183.9200  186.1900  46792908\n",
      "2024-01-11  186.5400  187.0500  183.6200  185.5900  49128408\n",
      "2024-01-12  186.0600  186.7400  185.1900  185.9200  40477782\n",
      "2024-01-16  182.1600  184.2600  180.9340  183.6300  65603041\n",
      "...              ...       ...       ...       ...       ...\n",
      "2024-05-24  188.8200  190.5800  188.0404  189.9800  36326975\n",
      "2024-05-28  191.5100  193.0000  189.1000  189.9900  52280051\n",
      "2024-05-29  189.6100  192.2470  189.5100  190.2900  53068016\n",
      "2024-05-30  190.7600  192.1800  190.6300  191.2900  49947941\n",
      "2024-05-31  191.4400  192.5700  189.9100  192.2500  75158277\n",
      "\n",
      "[100 rows x 5 columns]\n",
      "Data has been saved to aapl_stock_data.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# Replace with the actual API URL (including endpoint)\n",
    "url = \"https://www.alphavantage.co/query\"\n",
    "\n",
    "# Replace with your actual API key\n",
    "api_key = \"1EB6SE6BZ0SWH6PE\"\n",
    "\n",
    "# Parameters for the API call\n",
    "params = {\n",
    "    \"function\": \"TIME_SERIES_DAILY\",\n",
    "    \"symbol\": \"AAPL\",\n",
    "    \"apikey\": api_key\n",
    "}\n",
    "\n",
    "# Making the request\n",
    "response = requests.get(url, params=params)\n",
    "\n",
    "# Checking if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Parsing the JSON response\n",
    "    data = response.json()\n",
    "    \n",
    "    # Extracting the time series data\n",
    "    time_series = data.get(\"Time Series (Daily)\", {})\n",
    "    \n",
    "    # Converting the time series data to a DataFrame\n",
    "    df = pd.DataFrame.from_dict(time_series, orient='index')\n",
    "    \n",
    "    # Converting index to datetime\n",
    "    df.index = pd.to_datetime(df.index)\n",
    "    \n",
    "    # Sorting the DataFrame by index (date)\n",
    "    df.sort_index(inplace=True)\n",
    "    \n",
    "    # Renaming columns for better readability\n",
    "    df.columns = ['Open', 'High', 'Low', 'Close', 'Volume']\n",
    "    \n",
    "    # Displaying the DataFrame\n",
    "    print(df)\n",
    "    \n",
    "    # Saving the DataFrame to a CSV file\n",
    "    csv_file = \"aapl_stock_data.csv\"\n",
    "    df.to_csv(csv_file)\n",
    "    print(f\"Data has been saved to {csv_file}\")\n",
    "else:\n",
    "    print(f\"Error: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "800c5ad3-85af-4ce6-b38a-acd08e65f663",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                Open      High       Low     Close    Volume  Day  Month  Year\n",
      "2024-01-09  0.444751  0.424277  0.479108  0.587185 -0.828522    9      1  2024\n",
      "2024-01-10  0.494178  0.569939  0.617546  0.707769 -0.640191   10      1  2024\n",
      "2024-01-11  0.745914  0.645684  0.582646  0.638864 -0.528869   11      1  2024\n",
      "2024-01-12  0.690739  0.609559  0.765292  0.676762 -0.941204   12      1  2024\n",
      "2024-01-16  0.242443  0.320566  0.270170  0.413775  0.256402   16      1  2024\n",
      "...              ...       ...       ...       ...       ...  ...    ...   ...\n",
      "2024-05-24  1.007994  1.057034  1.096893  1.143019 -1.139054   24      5  2024\n",
      "2024-05-28  1.317204  1.339036  1.220162  1.144168 -0.378644   28      5  2024\n",
      "2024-05-29  1.098803  1.251289  1.267859  1.178620 -0.341086   29      5  2024\n",
      "2024-05-30  1.230993  1.243481  1.398155  1.293462 -0.489805   30      5  2024\n",
      "2024-05-31  1.309157  1.288928  1.314393  1.403710  0.711856   31      5  2024\n",
      "\n",
      "[100 rows x 8 columns]\n",
      "Data has been saved to aapl_stock_data.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Replace with the actual API URL (including endpoint)\n",
    "url = \"https://www.alphavantage.co/query\"\n",
    "\n",
    "# Replace with your actual API key\n",
    "api_key = \"1EB6SE6BZ0SWH6PE\"\n",
    "\n",
    "# Parameters for the API call\n",
    "params = {\n",
    "    \"function\": \"TIME_SERIES_DAILY\",\n",
    "    \"symbol\": \"AAPL\",\n",
    "    \"apikey\": api_key\n",
    "}\n",
    "\n",
    "# Making the request\n",
    "response = requests.get(url, params=params)\n",
    "\n",
    "# Checking if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Parsing the JSON response\n",
    "    data = response.json()\n",
    "    \n",
    "    # Extracting the time series data\n",
    "    time_series = data.get(\"Time Series (Daily)\", {})\n",
    "    \n",
    "    # Converting the time series data to a DataFrame\n",
    "    df = pd.DataFrame.from_dict(time_series, orient='index')\n",
    "    \n",
    "    # Converting index to datetime\n",
    "    df.index = pd.to_datetime(df.index)\n",
    "    \n",
    "    # Sorting the DataFrame by index (date)\n",
    "    df.sort_index(inplace=True)\n",
    "    \n",
    "    # Renaming columns for better readability\n",
    "    df.columns = ['Open', 'High', 'Low', 'Close', 'Volume']\n",
    "    \n",
    "    # Handle missing values (remove rows with missing values)\n",
    "    df.dropna(inplace=True)\n",
    "    \n",
    "    # Convert columns to numeric type\n",
    "    df = df.apply(pd.to_numeric)\n",
    "    \n",
    "    # Normalize/standardize numerical features\n",
    "    scaler = StandardScaler()\n",
    "    df[['Open', 'High', 'Low', 'Close', 'Volume']] = scaler.fit_transform(df[['Open', 'High', 'Low', 'Close', 'Volume']])\n",
    "    \n",
    "    # Extracting day, month, year from the index (date)\n",
    "    df['Day'] = df.index.day\n",
    "    df['Month'] = df.index.month\n",
    "    df['Year'] = df.index.year\n",
    "    \n",
    "    # Displaying the DataFrame\n",
    "    print(df)\n",
    "    \n",
    "    # Saving the DataFrame to a CSV file\n",
    "    csv_file = \"aapl_stock_data.csv\"\n",
    "    df.to_csv(csv_file)\n",
    "    print(f\"Data has been saved to {csv_file}\")\n",
    "else:\n",
    "    print(f\"Error: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "38182829-c64c-4415-a4bc-a2fae258bab7",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unterminated string literal (detected at line 186) (3361337511.py, line 186)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[11], line 186\u001b[1;36m\u001b[0m\n\u001b[1;33m    plt.plot(df.index, df['BB_lower'], label='Boll\u001b[0m\n\u001b[1;37m                                             ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unterminated string literal (detected at line 186)\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas_ta as ta\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n",
    "import numpy as np\n",
    "\n",
    "# Replace with the actual API URL (including endpoint)\n",
    "url = \"https://www.alphavantage.co/query\"\n",
    "\n",
    "# Replace with your actual API key\n",
    "api_key = \"1EB6SE6BZ0SWH6PE\"\n",
    "\n",
    "# Parameters for the API call\n",
    "params = {\n",
    "    \"function\": \"TIME_SERIES_DAILY\",\n",
    "    \"symbol\": \"AAPL\",\n",
    "    \"apikey\": api_key\n",
    "}\n",
    "\n",
    "# Making the request\n",
    "response = requests.get(url, params=params)\n",
    "\n",
    "# Checking if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Parsing the JSON response\n",
    "    data = response.json()\n",
    "    \n",
    "    # Extracting the time series data\n",
    "    time_series = data.get(\"Time Series (Daily)\", {})\n",
    "    \n",
    "    # Converting the time series data to a DataFrame\n",
    "    df = pd.DataFrame.from_dict(time_series, orient='index')\n",
    "    \n",
    "    # Converting index to datetime\n",
    "    df.index = pd.to_datetime(df.index)\n",
    "    \n",
    "    # Sorting the DataFrame by index (date)\n",
    "    df.sort_index(inplace=True)\n",
    "    \n",
    "    # Renaming columns for better readability\n",
    "    df.columns = ['Open', 'High', 'Low', 'Close', 'Volume']\n",
    "    \n",
    "    # Handle missing values (remove rows with missing values)\n",
    "    df.dropna(inplace=True)\n",
    "    \n",
    "    # Convert columns to numeric type\n",
    "    df = df.apply(pd.to_numeric)\n",
    "    \n",
    "    # Normalize/standardize numerical features\n",
    "    scaler = StandardScaler()\n",
    "    df[['Open', 'High', 'Low', 'Close', 'Volume']] = scaler.fit_transform(df[['Open', 'High', 'Low', 'Close', 'Volume']])\n",
    "    \n",
    "    # Extracting day, month, year from the index (date)\n",
    "    df['Day'] = df.index.day\n",
    "    df['Month'] = df.index.month\n",
    "    df['Year'] = df.index.year\n",
    "    \n",
    "    # Calculate technical indicators\n",
    "    df['SMA_50'] = ta.sma(df['Close'], length=50)\n",
    "    df['SMA_200'] = ta.sma(df['Close'], length=200)\n",
    "    df['RSI'] = ta.rsi(df['Close'], length=14)\n",
    "    df['MACD'] = ta.macd(df['Close'])['MACD_12_26_9']\n",
    "    df['MACD_Signal'] = ta.macd(df['Close'])['MACDs_12_26_9']\n",
    "    df['MACD_Hist'] = ta.macd(df['Close'])['MACDh_12_26_9']\n",
    "    df['BB_upper'], df['BB_middle'], df['BB_lower'] = ta.bbands(df['Close'], length=20, std=2)\n",
    "    \n",
    "    # Create lagged features\n",
    "    df['Lag_1'] = df['Close'].shift(1)\n",
    "    df['Lag_2'] = df['Close'].shift(2)\n",
    "    \n",
    "    # Calculate rolling statistics\n",
    "    df['Rolling_Mean_50'] = df['Close'].rolling(window=50).mean()\n",
    "    df['Rolling_Std_50'] = df['Close'].rolling(window=50).std()\n",
    "    \n",
    "    # Drop rows with NaN values generated from lag and rolling calculations\n",
    "    df.dropna(inplace=True)\n",
    "    \n",
    "    # Define the target variable and features\n",
    "    target = 'Close'\n",
    "    features = df.columns.drop(target)\n",
    "    \n",
    "    X = df[features]\n",
    "    y = df[target]\n",
    "    \n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "    \n",
    "    # Baseline Model: Linear Regression\n",
    "    lr = LinearRegression()\n",
    "    lr.fit(X_train, y_train)\n",
    "    y_pred_lr = lr.predict(X_test)\n",
    "    print(f\"Linear Regression MSE: {mean_squared_error(y_test, y_pred_lr)}\")\n",
    "    \n",
    "    # Advanced Model: Decision Tree with Grid Search\n",
    "    dt = DecisionTreeRegressor()\n",
    "    dt_params = {'max_depth': [5, 10, 15], 'min_samples_split': [2, 5, 10]}\n",
    "    dt_grid = GridSearchCV(dt, dt_params, cv=5)\n",
    "    dt_grid.fit(X_train, y_train)\n",
    "    y_pred_dt = dt_grid.predict(X_test)\n",
    "    print(f\"Decision Tree MSE: {mean_squared_error(y_test, y_pred_dt)}\")\n",
    "    \n",
    "    # Advanced Model: Random Forest with Random Search\n",
    "    rf = RandomForestRegressor()\n",
    "    rf_params = {'n_estimators': [50, 100, 200], 'max_features': ['auto', 'sqrt', 'log2'], 'max_depth': [5, 10, 15]}\n",
    "    rf_random = RandomizedSearchCV(rf, rf_params, cv=5, n_iter=10)\n",
    "    rf_random.fit(X_train, y_train)\n",
    "    y_pred_rf = rf_random.predict(X_test)\n",
    "    print(f\"Random Forest MSE: {mean_squared_error(y_test, y_pred_rf)}\")\n",
    "    \n",
    "    # Advanced Model: Gradient Boosting with Grid Search\n",
    "    gb = GradientBoostingRegressor()\n",
    "    gb_params = {'n_estimators': [50, 100, 200], 'learning_rate': [0.01, 0.1, 0.2], 'max_depth': [3, 5, 7]}\n",
    "    gb_grid = GridSearchCV(gb, gb_params, cv=5)\n",
    "    gb_grid.fit(X_train, y_train)\n",
    "    y_pred_gb = gb_grid.predict(X_test)\n",
    "    print(f\"Gradient Boosting MSE: {mean_squared_error(y_test, y_pred_gb)}\")\n",
    "    \n",
    "    # Time Series Model: ARIMA\n",
    "    arima = ARIMA(y_train, order=(5, 1, 0))\n",
    "    arima_fit = arima.fit()\n",
    "    y_pred_arima = arima_fit.forecast(steps=len(y_test))\n",
    "    print(f\"ARIMA MSE: {mean_squared_error(y_test, y_pred_arima)}\")\n",
    "    \n",
    "    # Time Series Model: LSTM\n",
    "    n_input = 5\n",
    "    n_features = len(features)\n",
    "    generator = TimeseriesGenerator(X_train, y_train, length=n_input, batch_size=1)\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(LSTM(100, activation='relu', input_shape=(n_input, n_features)))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    model.fit(generator, epochs=50)\n",
    "    \n",
    "    # Preparing test data for LSTM prediction\n",
    "    test_generator = TimeseriesGenerator(X_test, y_test, length=n_input, batch_size=1)\n",
    "    y_pred_lstm = model.predict(test_generator)\n",
    "    y_test_lstm = y_test[n_input:]\n",
    "    print(f\"LSTM MSE: {mean_squared_error(y_test_lstm, y_pred_lstm)}\")\n",
    "    \n",
    "    # Plot time series of stock prices\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    plt.plot(df.index, df['Close'], label='Close Price')\n",
    "    plt.title('AAPL Stock Close Price')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Normalized Price')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Plot technical indicators\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    plt.plot(df.index, df['Close'], label='Close Price')\n",
    "    plt.plot(df.index, df['SMA_50'], label='50-day SMA')\n",
    "    plt.plot(df.index, df['SMA_200'], label='200-day SMA')\n",
    "    plt.title('AAPL Stock Close Price and Moving Averages')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Normalized Price')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    plt.figure(figsize=(14, 7))\n",
    "    plt.plot(df.index, df['MACD'], label='MACD')\n",
    "    plt.plot(df.index, df['MACD_Signal'], label='MACD Signal')\n",
    "    plt.bar(df.index, df['MACD_Hist'], label='MACD Histogram')\n",
    "    plt.title('AAPL MACD')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Value')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    plt.plot(df.index, df['Close'], label='Close Price')\n",
    "    plt.plot(df.index, df['BB_upper'], label='Bollinger Band Upper')\n",
    "    plt.plot(df.index, df['BB_middle'], label='Bollinger Band Middle')\n",
    "    plt.plot(df.index, df['BB_lower'], label='Boll\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0ed0c30e-924f-411f-8afa-2ebda09bdcf6",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mean_squared_error\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mstatsmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtsa\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marima\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ARIMA\n\u001b[1;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Sequential\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LSTM, Dense\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msequence\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TimeseriesGenerator\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as ta\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n",
    "import numpy as np\n",
    "\n",
    "# Replace with the actual API URL (including endpoint)\n",
    "url = \"https://www.alphavantage.co/query\"\n",
    "\n",
    "# Replace with your actual API key\n",
    "api_key = \"1EB6SE6BZ0SWH6PE\"\n",
    "\n",
    "# Parameters for the API call\n",
    "params = {\n",
    "    \"function\": \"TIME_SERIES_DAILY\",\n",
    "    \"symbol\": \"AAPL\",\n",
    "    \"apikey\": api_key\n",
    "}\n",
    "\n",
    "# Making the request\n",
    "response = requests.get(url, params=params)\n",
    "\n",
    "# Checking if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Parsing the JSON response\n",
    "    data = response.json()\n",
    "    \n",
    "    # Extracting the time series data\n",
    "    time_series = data.get(\"Time Series (Daily)\", {})\n",
    "    \n",
    "    # Converting the time series data to a DataFrame\n",
    "    df = pd.DataFrame.from_dict(time_series, orient='index')\n",
    "    \n",
    "    # Converting index to datetime\n",
    "    df.index = pd.to_datetime(df.index)\n",
    "    \n",
    "    # Sorting the DataFrame by index (date)\n",
    "    df.sort_index(inplace=True)\n",
    "    \n",
    "    # Renaming columns for better readability\n",
    "    df.columns = ['Open', 'High', 'Low', 'Close', 'Volume']\n",
    "    \n",
    "    # Handle missing values (remove rows with missing values)\n",
    "    df.dropna(inplace=True)\n",
    "    \n",
    "    # Convert columns to numeric type\n",
    "    df = df.apply(pd.to_numeric)\n",
    "    \n",
    "    # Normalize/standardize numerical features\n",
    "    scaler = StandardScaler()\n",
    "    df[['Open', 'High', 'Low', 'Close', 'Volume']] = scaler.fit_transform(df[['Open', 'High', 'Low', 'Close', 'Volume']])\n",
    "    \n",
    "    # Extracting day, month, year from the index (date)\n",
    "    df['Day'] = df.index.day\n",
    "    df['Month'] = df.index.month\n",
    "    df['Year'] = df.index.year\n",
    "    \n",
    "    # Calculate technical indicators\n",
    "    df['SMA_50'] = ta.sma(df['Close'], length=50)\n",
    "    df['SMA_200'] = ta.sma(df['Close'], length=200)\n",
    "    df['RSI'] = ta.rsi(df['Close'], length=14)\n",
    "    df['MACD'] = ta.macd(df['Close'])['MACD_12_26_9']\n",
    "    df['MACD_Signal'] = ta.macd(df['Close'])['MACDs_12_26_9']\n",
    "    df['MACD_Hist'] = ta.macd(df['Close'])['MACDh_12_26_9']\n",
    "    df['BB_upper'], df['BB_middle'], df['BB_lower'] = ta.bbands(df['Close'], length=20, std=2)\n",
    "    \n",
    "    # Create lagged features\n",
    "    df['Lag_1'] = df['Close'].shift(1)\n",
    "    df['Lag_2'] = df['Close'].shift(2)\n",
    "    \n",
    "    # Calculate rolling statistics\n",
    "    df['Rolling_Mean_50'] = df['Close'].rolling(window=50).mean()\n",
    "    df['Rolling_Std_50'] = df['Close'].rolling(window=50).std()\n",
    "    \n",
    "    # Drop rows with NaN values generated from lag and rolling calculations\n",
    "    df.dropna(inplace=True)\n",
    "    \n",
    "    # Define the target variable and features\n",
    "    target = 'Close'\n",
    "    features = df.columns.drop(target)\n",
    "    \n",
    "    X = df[features]\n",
    "    y = df[target]\n",
    "    \n",
    "    # Split the data into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "    \n",
    "    # Baseline Model: Linear Regression\n",
    "    lr = LinearRegression()\n",
    "    lr.fit(X_train, y_train)\n",
    "    y_pred_lr = lr.predict(X_test)\n",
    "    print(f\"Linear Regression MSE: {mean_squared_error(y_test, y_pred_lr)}\")\n",
    "    \n",
    "    # Advanced Model: Decision Tree\n",
    "    dt = DecisionTreeRegressor()\n",
    "    dt.fit(X_train, y_train)\n",
    "    y_pred_dt = dt.predict(X_test)\n",
    "    print(f\"Decision Tree MSE: {mean_squared_error(y_test, y_pred_dt)}\")\n",
    "    \n",
    "    # Advanced Model: Random Forest\n",
    "    rf = RandomForestRegressor()\n",
    "    rf.fit(X_train, y_train)\n",
    "    y_pred_rf = rf.predict(X_test)\n",
    "    print(f\"Random Forest MSE: {mean_squared_error(y_test, y_pred_rf)}\")\n",
    "    \n",
    "    # Advanced Model: Gradient Boosting\n",
    "    gb = GradientBoostingRegressor()\n",
    "    gb.fit(X_train, y_train)\n",
    "    y_pred_gb = gb.predict(X_test)\n",
    "    print(f\"Gradient Boosting MSE: {mean_squared_error(y_test, y_pred_gb)}\")\n",
    "    \n",
    "    # Time Series Model: ARIMA\n",
    "    arima = ARIMA(y_train, order=(5, 1, 0))\n",
    "    arima_fit = arima.fit()\n",
    "    y_pred_arima = arima_fit.forecast(steps=len(y_test))\n",
    "    print(f\"ARIMA MSE: {mean_squared_error(y_test, y_pred_arima)}\")\n",
    "    \n",
    "    # Time Series Model: LSTM\n",
    "    n_input = 5\n",
    "    n_features = len(features)\n",
    "    generator = TimeseriesGenerator(X_train, y_train, length=n_input, batch_size=1)\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(LSTM(100, activation='relu', input_shape=(n_input, n_features)))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    model.fit(generator, epochs=50)\n",
    "    \n",
    "    # Preparing test data for LSTM prediction\n",
    "    test_generator = TimeseriesGenerator(X_test, y_test, length=n_input, batch_size=1)\n",
    "    y_pred_lstm = model.predict(test_generator)\n",
    "    y_test_lstm = y_test[n_input:]\n",
    "    print(f\"LSTM MSE: {mean_squared_error(y_test_lstm, y_pred_lstm)}\")\n",
    "    \n",
    "    # Plot time series of stock prices\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    plt.plot(df.index, df['Close'], label='Close Price')\n",
    "    plt.title('AAPL Stock Close Price')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Normalized Price')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Plot technical indicators\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    plt.plot(df.index, df['Close'], label='Close Price')\n",
    "    plt.plot(df.index, df['SMA_50'], label='50-day SMA')\n",
    "    plt.plot(df.index, df['SMA_200'], label='200-day SMA')\n",
    "    plt.title('AAPL Stock Close Price and Moving Averages')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Normalized Price')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    plt.figure(figsize=(14, 7))\n",
    "    plt.plot(df.index, df['MACD'], label='MACD')\n",
    "    plt.plot(df.index, df['MACD_Signal'], label='MACD Signal')\n",
    "    plt.bar(df.index, df['MACD_Hist'], label='MACD Histogram')\n",
    "    plt.title('AAPL MACD')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Value')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    plt.plot(df.index, df['Close'], label='Close Price')\n",
    "    plt.plot(df.index, df['BB_upper'], label='Bollinger Band Upper')\n",
    "    plt.plot(df.index, df['BB_middle'], label='Bollinger Band Middle')\n",
    "    plt.plot(df.index, df['BB_lower'], label='Bollinger Band Lower')\n",
    "    plt.title('AAPL Bollinger Bands')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Normalized Price')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    # Analyze correlations between different features\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.heatmap(df.corr(), annot=True, cmap='coolwarm', center=0)\n",
    "    plt.title('Feature Correlations')\n",
    "    plt.show()\n",
    "    \n",
    "    # Visualize distributions and trends\n",
    "    df[['Open', 'High', 'Low', 'Close', 'Volume']].hist(bins=50, figsize=(20, 15))\n",
    "    plt.suptitle('Feature Distributions')\n",
    "    plt.show()\n",
    "    \n",
    "    # Saving the DataFrame to a CSV file\n",
    "    csv_file = \"aapl_stock_data.csv\"\n",
    "    df.to_csv(csv_file)\n",
    "    print(f\"Data has been saved to {csv_file}\")\n",
    "else:\n",
    "    print(f\"Error: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2870a3a-372b-4849-a1f2-871d5a5a58da",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 77\u001b[0m\n\u001b[0;32m     75\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMACD_Signal\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m ta\u001b[38;5;241m.\u001b[39mmacd(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mClose\u001b[39m\u001b[38;5;124m'\u001b[39m])[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMACDs_12_26_9\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     76\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMACD_Hist\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m ta\u001b[38;5;241m.\u001b[39mmacd(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mClose\u001b[39m\u001b[38;5;124m'\u001b[39m])[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMACDh_12_26_9\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m---> 77\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBB_upper\u001b[39m\u001b[38;5;124m'\u001b[39m], df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBB_middle\u001b[39m\u001b[38;5;124m'\u001b[39m], df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBB_lower\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m ta\u001b[38;5;241m.\u001b[39mbbands(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mClose\u001b[39m\u001b[38;5;124m'\u001b[39m], length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m, std\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m     79\u001b[0m \u001b[38;5;66;03m# Create lagged features\u001b[39;00m\n\u001b[0;32m     80\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLag_1\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mClose\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mshift(\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 3)"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas_ta as ta\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n",
    "import numpy as np\n",
    "\n",
    "# Replace with the actual API URL (including endpoint)\n",
    "url = \"https://www.alphavantage.co/query\"\n",
    "\n",
    "# Replace with your actual API key\n",
    "api_key = \"1EB6SE6BZ0SWH6PE\"\n",
    "\n",
    "# Parameters for the API call\n",
    "params = {\n",
    "    \"function\": \"TIME_SERIES_DAILY\",\n",
    "    \"symbol\": \"AAPL\",\n",
    "    \"apikey\": api_key\n",
    "}\n",
    "\n",
    "# Making the request\n",
    "response = requests.get(url, params=params)\n",
    "\n",
    "# Checking if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Parsing the JSON response\n",
    "    data = response.json()\n",
    "    \n",
    "    # Extracting the time series data\n",
    "    time_series = data.get(\"Time Series (Daily)\", {})\n",
    "    \n",
    "    # Converting the time series data to a DataFrame\n",
    "    df = pd.DataFrame.from_dict(time_series, orient='index')\n",
    "    \n",
    "    # Converting index to datetime\n",
    "    df.index = pd.to_datetime(df.index)\n",
    "    \n",
    "    # Sorting the DataFrame by index (date)\n",
    "    df.sort_index(inplace=True)\n",
    "    \n",
    "    # Renaming columns for better readability\n",
    "    df.columns = ['Open', 'High', 'Low', 'Close', 'Volume']\n",
    "    \n",
    "    # Handle missing values (remove rows with missing values)\n",
    "    df.dropna(inplace=True)\n",
    "    \n",
    "    # Convert columns to numeric type\n",
    "    df = df.apply(pd.to_numeric)\n",
    "    \n",
    "    # Normalize/standardize numerical features\n",
    "    scaler = StandardScaler()\n",
    "    df[['Open', 'High', 'Low', 'Close', 'Volume']] = scaler.fit_transform(df[['Open', 'High', 'Low', 'Close', 'Volume']])\n",
    "    \n",
    "    # Extracting day, month, year from the index (date)\n",
    "    df['Day'] = df.index.day\n",
    "    df['Month'] = df.index.month\n",
    "    df['Year'] = df.index.year\n",
    "    \n",
    "    # Calculate technical indicators\n",
    "    df['SMA_50'] = ta.sma(df['Close'], length=50)\n",
    "    df['SMA_200'] = ta.sma(df['Close'], length=200)\n",
    "    df['RSI'] = ta.rsi(df['Close'], length=14)\n",
    "    df['MACD'] = ta.macd(df['Close'])['MACD_12_26_9']\n",
    "    df['MACD_Signal'] = ta.macd(df['Close'])['MACDs_12_26_9']\n",
    "    df['MACD_Hist'] = ta.macd(df['Close'])['MACDh_12_26_9']\n",
    "    df['BB_upper'], df['BB_middle'], df['BB_lower'] = ta.bbands(df['Close'], length=20, std=2)\n",
    "    \n",
    "    # Create lagged features\n",
    "    df['Lag_1'] = df['Close'].shift(1)\n",
    "    df['Lag_2'] = df['Close'].shift(2)\n",
    "    \n",
    "    # Calculate rolling statistics\n",
    "    df['Rolling_Mean_50'] = df['Close'].rolling(window=50).mean()\n",
    "    df['Rolling_Std_50'] = df['Close'].rolling(window=50).std()\n",
    "    \n",
    "    # Drop rows with NaN values generated from lag and rolling calculations\n",
    "    df.dropna(inplace=True)\n",
    "    \n",
    "    # Define the target variable and features\n",
    "    target = 'Close'\n",
    "    features = df.columns.drop(target)\n",
    "    \n",
    "    X = df[features]\n",
    "    y = df[target]\n",
    "    \n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "    \n",
    "    # Baseline Model: Linear Regression\n",
    "    lr = LinearRegression()\n",
    "    lr.fit(X_train, y_train)\n",
    "    y_pred_lr = lr.predict(X_test)\n",
    "    print(f\"Linear Regression MSE: {mean_squared_error(y_test, y_pred_lr)}\")\n",
    "    \n",
    "    # Advanced Model: Decision Tree with Grid Search\n",
    "    dt = DecisionTreeRegressor()\n",
    "    dt_params = {'max_depth': [5, 10, 15], 'min_samples_split': [2, 5, 10]}\n",
    "    dt_grid = GridSearchCV(dt, dt_params, cv=5)\n",
    "    dt_grid.fit(X_train, y_train)\n",
    "    y_pred_dt = dt_grid.predict(X_test)\n",
    "    print(f\"Decision Tree MSE: {mean_squared_error(y_test, y_pred_dt)}\")\n",
    "    \n",
    "    # Advanced Model: Random Forest with Random Search\n",
    "    rf = RandomForestRegressor()\n",
    "    rf_params = {'n_estimators': [50, 100, 200], 'max_features': ['auto', 'sqrt', 'log2'], 'max_depth': [5, 10, 15]}\n",
    "    rf_random = RandomizedSearchCV(rf, rf_params, cv=5, n_iter=10)\n",
    "    rf_random.fit(X_train, y_train)\n",
    "    y_pred_rf = rf_random.predict(X_test)\n",
    "    print(f\"Random Forest MSE: {mean_squared_error(y_test, y_pred_rf)}\")\n",
    "    \n",
    "    # Advanced Model: Gradient Boosting with Grid Search\n",
    "    gb = GradientBoostingRegressor()\n",
    "    gb_params = {'n_estimators': [50, 100, 200], 'learning_rate': [0.01, 0.1, 0.2], 'max_depth': [3, 5, 7]}\n",
    "    gb_grid = GridSearchCV(gb, gb_params, cv=5)\n",
    "    gb_grid.fit(X_train, y_train)\n",
    "    y_pred_gb = gb_grid.predict(X_test)\n",
    "    print(f\"Gradient Boosting MSE: {mean_squared_error(y_test, y_pred_gb)}\")\n",
    "    \n",
    "    # Time Series Model: ARIMA\n",
    "    arima = ARIMA(y_train, order=(5, 1, 0))\n",
    "    arima_fit = arima.fit()\n",
    "    y_pred_arima = arima_fit.forecast(steps=len(y_test))\n",
    "    print(f\"ARIMA MSE: {mean_squared_error(y_test, y_pred_arima)}\")\n",
    "    \n",
    "    # Time Series Model: LSTM\n",
    "    n_input = 5\n",
    "    n_features = len(features)\n",
    "    generator = TimeseriesGenerator(X_train, y_train, length=n_input, batch_size=1)\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(LSTM(100, activation='relu', input_shape=(n_input, n_features)))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    model.fit(generator, epochs=50)\n",
    "    \n",
    "    # Preparing test data for LSTM prediction\n",
    "    test_generator = TimeseriesGenerator(X_test, y_test, length=n_input, batch_size=1)\n",
    "    y_pred_lstm = model.predict(test_generator)\n",
    "    y_test_lstm = y_test[n_input:]\n",
    "    print(f\"LSTM MSE: {mean_squared_error(y_test_lstm, y_pred_lstm)}\")\n",
    "    \n",
    "    # Plot time series of stock prices\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    plt.plot(df.index, df['Close'], label='Close Price')\n",
    "    plt.title('AAPL Stock Close Price')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Normalized Price')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Plot technical indicators\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    plt.plot(df.index, df['Close'], label='Close Price')\n",
    "    plt.plot(df.index, df['SMA_50'], label='50-day SMA')\n",
    "    plt.plot(df.index, df['SMA_200'], label='200-day SMA')\n",
    "    plt.title('AAPL Stock Close Price and Moving Averages')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Normalized Price')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    plt.figure(figsize=(14, 7))\n",
    "    plt.plot(df.index, df['MACD'], label='MACD')\n",
    "    plt.plot(df.index, df['MACD_Signal'], label='MACD Signal')\n",
    "    plt.bar(df.index, df['MACD_Hist'], label='MACD Histogram')\n",
    "    plt.title('AAPL MACD')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Value')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    plt.plot(df.index, df['Close'], label='Close Price')\n",
    "    plt.plot(df.index, df['BB_upper'], label='Bollinger Band Upper')\n",
    "    plt.plot(df.index, df['BB_middle'], label='Bollinger Band Middle')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aff1071c-d373-4e4d-a228-5b6a89e624f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas_ta\n",
      "  Downloading pandas_ta-0.3.14b.tar.gz (115 kB)\n",
      "     ---------------------------------------- 0.0/115.1 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/115.1 kB ? eta -:--:--\n",
      "     --- ------------------------------------ 10.2/115.1 kB ? eta -:--:--\n",
      "     ------------- ----------------------- 41.0/115.1 kB 393.8 kB/s eta 0:00:01\n",
      "     -----------------------------------  112.6/115.1 kB 819.2 kB/s eta 0:00:01\n",
      "     ------------------------------------ 115.1/115.1 kB 748.9 kB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: pandas in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas_ta) (2.2.0)\n",
      "Requirement already satisfied: numpy<2,>=1.23.2 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas->pandas_ta) (1.26.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas->pandas_ta) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas->pandas_ta) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas->pandas_ta) (2023.4)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->pandas_ta) (1.16.0)\n",
      "Building wheels for collected packages: pandas_ta\n",
      "  Building wheel for pandas_ta (setup.py): started\n",
      "  Building wheel for pandas_ta (setup.py): finished with status 'done'\n",
      "  Created wheel for pandas_ta: filename=pandas_ta-0.3.14b0-py3-none-any.whl size=218924 sha256=40228ca3a8806a427ca16e90f789d114b7982b11b0b84f1025fe1718b5a3d8f5\n",
      "  Stored in directory: c:\\users\\asus\\appdata\\local\\pip\\cache\\wheels\\7f\\33\\8b\\50b245c5c65433cd8f5cb24ac15d97e5a3db2d41a8b6ae957d\n",
      "Successfully built pandas_ta\n",
      "Installing collected packages: pandas_ta\n",
      "Successfully installed pandas_ta-0.3.14b0\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas_ta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "80a0b3e4-a75e-451c-8e22-502609fc67e9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "With n_samples=0, test_size=0.2 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 102\u001b[0m\n\u001b[0;32m     99\u001b[0m y \u001b[38;5;241m=\u001b[39m df[target]\n\u001b[0;32m    101\u001b[0m \u001b[38;5;66;03m# Split the data into training and testing sets\u001b[39;00m\n\u001b[1;32m--> 102\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_test_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;66;03m# Baseline Model: Linear Regression\u001b[39;00m\n\u001b[0;32m    105\u001b[0m lr \u001b[38;5;241m=\u001b[39m LinearRegression()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n\u001b[1;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    223\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:2660\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[1;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[0;32m   2657\u001b[0m arrays \u001b[38;5;241m=\u001b[39m indexable(\u001b[38;5;241m*\u001b[39marrays)\n\u001b[0;32m   2659\u001b[0m n_samples \u001b[38;5;241m=\u001b[39m _num_samples(arrays[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m-> 2660\u001b[0m n_train, n_test \u001b[38;5;241m=\u001b[39m \u001b[43m_validate_shuffle_split\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2661\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefault_test_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.25\u001b[39;49m\n\u001b[0;32m   2662\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2664\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m shuffle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m   2665\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stratify \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:2308\u001b[0m, in \u001b[0;36m_validate_shuffle_split\u001b[1;34m(n_samples, test_size, train_size, default_test_size)\u001b[0m\n\u001b[0;32m   2305\u001b[0m n_train, n_test \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(n_train), \u001b[38;5;28mint\u001b[39m(n_test)\n\u001b[0;32m   2307\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_train \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 2308\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2309\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWith n_samples=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, test_size=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m and train_size=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2310\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresulting train set will be empty. Adjust any of the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2311\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maforementioned parameters.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(n_samples, test_size, train_size)\n\u001b[0;32m   2312\u001b[0m     )\n\u001b[0;32m   2314\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m n_train, n_test\n",
      "\u001b[1;31mValueError\u001b[0m: With n_samples=0, test_size=0.2 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters."
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas_ta as ta\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n",
    "import numpy as np\n",
    "\n",
    "# Replace with the actual API URL (including endpoint)\n",
    "url = \"https://www.alphavantage.co/query\"\n",
    "\n",
    "# Replace with your actual API key\n",
    "api_key = \"1EB6SE6BZ0SWH6PE\"\n",
    "\n",
    "# Parameters for the API call\n",
    "params = {\n",
    "    \"function\": \"TIME_SERIES_DAILY\",\n",
    "    \"symbol\": \"AAPL\",\n",
    "    \"apikey\": api_key\n",
    "}\n",
    "\n",
    "# Making the request\n",
    "response = requests.get(url, params=params)\n",
    "\n",
    "# Checking if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Parsing the JSON response\n",
    "    data = response.json()\n",
    "    \n",
    "    # Extracting the time series data\n",
    "    time_series = data.get(\"Time Series (Daily)\", {})\n",
    "    \n",
    "    # Converting the time series data to a DataFrame\n",
    "    df = pd.DataFrame.from_dict(time_series, orient='index')\n",
    "    \n",
    "    # Converting index to datetime\n",
    "    df.index = pd.to_datetime(df.index)\n",
    "    \n",
    "    # Sorting the DataFrame by index (date)\n",
    "    df.sort_index(inplace=True)\n",
    "    \n",
    "    # Renaming columns for better readability\n",
    "    df.columns = ['Open', 'High', 'Low', 'Close', 'Volume']\n",
    "    \n",
    "    # Handle missing values (remove rows with missing values)\n",
    "    df.dropna(inplace=True)\n",
    "    \n",
    "    # Convert columns to numeric type\n",
    "    df = df.apply(pd.to_numeric)\n",
    "    \n",
    "    # Normalize/standardize numerical features\n",
    "    scaler = StandardScaler()\n",
    "    df[['Open', 'High', 'Low', 'Close', 'Volume']] = scaler.fit_transform(df[['Open', 'High', 'Low', 'Close', 'Volume']])\n",
    "    \n",
    "    # Extracting day, month, year from the index (date)\n",
    "    df['Day'] = df.index.day\n",
    "    df['Month'] = df.index.month\n",
    "    df['Year'] = df.index.year\n",
    "    \n",
    "    # Calculate technical indicators\n",
    "    df['SMA_50'] = ta.sma(df['Close'], length=50)\n",
    "    df['SMA_200'] = ta.sma(df['Close'], length=200)\n",
    "    df['RSI'] = ta.rsi(df['Close'], length=14)\n",
    "    df['MACD'] = ta.macd(df['Close'])['MACD_12_26_9']\n",
    "    df['MACD_Signal'] = ta.macd(df['Close'])['MACDs_12_26_9']\n",
    "    df['MACD_Hist'] = ta.macd(df['Close'])['MACDh_12_26_9']\n",
    "    \n",
    "    bbands = ta.bbands(df['Close'], length=20, std=2)\n",
    "    df['BB_upper'] = bbands['BBU_20_2.0']\n",
    "    df['BB_middle'] = bbands['BBM_20_2.0']\n",
    "    df['BB_lower'] = bbands['BBL_20_2.0']\n",
    "    \n",
    "    # Create lagged features\n",
    "    df['Lag_1'] = df['Close'].shift(1)\n",
    "    df['Lag_2'] = df['Close'].shift(2)\n",
    "    \n",
    "    # Calculate rolling statistics\n",
    "    df['Rolling_Mean_50'] = df['Close'].rolling(window=50).mean()\n",
    "    df['Rolling_Std_50'] = df['Close'].rolling(window=50).std()\n",
    "    \n",
    "    # Drop rows with NaN values generated from lag and rolling calculations\n",
    "    df.dropna(inplace=True)\n",
    "    \n",
    "    # Define the target variable and features\n",
    "    target = 'Close'\n",
    "    features = df.columns.drop(target)\n",
    "    \n",
    "    X = df[features]\n",
    "    y = df[target]\n",
    "    \n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "    \n",
    "    # Baseline Model: Linear Regression\n",
    "    lr = LinearRegression()\n",
    "    lr.fit(X_train, y_train)\n",
    "    y_pred_lr = lr.predict(X_test)\n",
    "    print(f\"Linear Regression MSE: {mean_squared_error(y_test, y_pred_lr)}\")\n",
    "    \n",
    "    # Advanced Model: Decision Tree with Grid Search\n",
    "    dt = DecisionTreeRegressor()\n",
    "    dt_params = {'max_depth': [5, 10, 15], 'min_samples_split': [2, 5, 10]}\n",
    "    dt_grid = GridSearchCV(dt, dt_params, cv=5)\n",
    "    dt_grid.fit(X_train, y_train)\n",
    "    y_pred_dt = dt_grid.predict(X_test)\n",
    "    print(f\"Decision Tree MSE: {mean_squared_error(y_test, y_pred_dt)}\")\n",
    "    \n",
    "    # Advanced Model: Random Forest with Random Search\n",
    "    rf = RandomForestRegressor()\n",
    "    rf_params = {'n_estimators': [50, 100, 200], 'max_features': ['auto', 'sqrt', 'log2'], 'max_depth': [5, 10, 15]}\n",
    "    rf_random = RandomizedSearchCV(rf, rf_params, cv=5, n_iter=10)\n",
    "    rf_random.fit(X_train, y_train)\n",
    "    y_pred_rf = rf_random.predict(X_test)\n",
    "    print(f\"Random Forest MSE: {mean_squared_error(y_test, y_pred_rf)}\")\n",
    "    \n",
    "    # Advanced Model: Gradient Boosting with Grid Search\n",
    "    gb = GradientBoostingRegressor()\n",
    "    gb_params = {'n_estimators': [50, 100, 200], 'learning_rate': [0.01, 0.1, 0.2], 'max_depth': [3, 5, 7]}\n",
    "    gb_grid = GridSearchCV(gb, gb_params, cv=5)\n",
    "    gb_grid.fit(X_train, y_train)\n",
    "    y_pred_gb = gb_grid.predict(X_test)\n",
    "    print(f\"Gradient Boosting MSE: {mean_squared_error(y_test, y_pred_gb)}\")\n",
    "    \n",
    "    # Time Series Model: ARIMA\n",
    "    arima = ARIMA(y_train, order=(5, 1, 0))\n",
    "    arima_fit = arima.fit()\n",
    "    y_pred_arima = arima_fit.forecast(steps=len(y_test))\n",
    "    print(f\"ARIMA MSE: {mean_squared_error(y_test, y_pred_arima)}\")\n",
    "    \n",
    "    # Time Series Model: LSTM\n",
    "    n_input = 5\n",
    "    n_features = len(features)\n",
    "    generator = TimeseriesGenerator(X_train, y_train, length=n_input, batch_size=1)\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(LSTM(100, activation='relu', input_shape=(n_input, n_features)))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    model.fit(generator, epochs=50)\n",
    "    \n",
    "    # Preparing test data for LSTM prediction\n",
    "    test_generator = TimeseriesGenerator(X_test, y_test, length=n_input, batch_size=1)\n",
    "    y_pred_lstm = model.predict(test_generator)\n",
    "    y_test_lstm = y_test[n_input:]\n",
    "    print(f\"LSTM MSE: {mean_squared_error(y_test_lstm, y_pred_lstm)}\")\n",
    "    \n",
    "    # Plot time series of stock prices\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    plt.plot(df.index, df['Close'], label='Close Price')\n",
    "    plt.title('AAPL Stock Close Price')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Normalized Price')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Plot technical indicators\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    plt.plot(df.index, df['Close'], label='Close Price')\n",
    "    plt.plot(df.index, df['SMA_50'], label='50-day SMA')\n",
    "    plt.plot(df.index, df['SMA_200'], label='200-day SMA')\n",
    "    plt.title('AAPL Stock Close Price and Moving Averages')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Normalized Price')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    plt.figure(figsize=(14, 7))\n",
    "    plt.plot(df.index, df['MACD'], label='MACD')\n",
    "    plt.plot(df.index, df['MACD_Signal'], label='MACD Signal')\n",
    "    plt.bar(df.index, df['MACD_Hist'], label='MACD Histogram')\n",
    "    plt.title('AAPL MACD')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Value')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    plt.plot(df.index, df['Close'], label='Close Price')\n",
    "    plt.plot(df.index, df['BB_upper'], label='Bollinger Band Upper')\n",
    "    plt.plot(df.index, df['BB_middle'], label='Bollinger Band Middle')\n",
    "    plt.plot(df.index, df['BB_lower'], label='Bollinger Band Lower')\n",
    "    plt.title('AAPL Bollinger Bands')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Normalized Price')\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "97b59e15-f3a1-468e-bc5e-1eeea37301c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples after preprocessing: 0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No samples available after preprocessing. Adjust the preprocessing steps or check the data.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 99\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;66;03m# Ensure there are enough samples\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m df\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m---> 99\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo samples available after preprocessing. Adjust the preprocessing steps or check the data.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    101\u001b[0m \u001b[38;5;66;03m# Define the target variable and features\u001b[39;00m\n\u001b[0;32m    102\u001b[0m target \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mClose\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[1;31mValueError\u001b[0m: No samples available after preprocessing. Adjust the preprocessing steps or check the data."
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas_ta as ta\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n",
    "import numpy as np\n",
    "\n",
    "# Replace with the actual API URL (including endpoint)\n",
    "url = \"https://www.alphavantage.co/query\"\n",
    "\n",
    "# Replace with your actual API key\n",
    "api_key = \"1EB6SE6BZ0SWH6PE\"\n",
    "\n",
    "# Parameters for the API call\n",
    "params = {\n",
    "    \"function\": \"TIME_SERIES_DAILY\",\n",
    "    \"symbol\": \"AAPL\",\n",
    "    \"apikey\": api_key\n",
    "}\n",
    "\n",
    "# Making the request\n",
    "response = requests.get(url, params=params)\n",
    "\n",
    "# Checking if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Parsing the JSON response\n",
    "    data = response.json()\n",
    "    \n",
    "    # Extracting the time series data\n",
    "    time_series = data.get(\"Time Series (Daily)\", {})\n",
    "    \n",
    "    # Converting the time series data to a DataFrame\n",
    "    df = pd.DataFrame.from_dict(time_series, orient='index')\n",
    "    \n",
    "    # Converting index to datetime\n",
    "    df.index = pd.to_datetime(df.index)\n",
    "    \n",
    "    # Sorting the DataFrame by index (date)\n",
    "    df.sort_index(inplace=True)\n",
    "    \n",
    "    # Renaming columns for better readability\n",
    "    df.columns = ['Open', 'High', 'Low', 'Close', 'Volume']\n",
    "    \n",
    "    # Handle missing values (remove rows with missing values)\n",
    "    df.dropna(inplace=True)\n",
    "    \n",
    "    # Convert columns to numeric type\n",
    "    df = df.apply(pd.to_numeric)\n",
    "    \n",
    "    # Normalize/standardize numerical features\n",
    "    scaler = StandardScaler()\n",
    "    df[['Open', 'High', 'Low', 'Close', 'Volume']] = scaler.fit_transform(df[['Open', 'High', 'Low', 'Close', 'Volume']])\n",
    "    \n",
    "    # Extracting day, month, year from the index (date)\n",
    "    df['Day'] = df.index.day\n",
    "    df['Month'] = df.index.month\n",
    "    df['Year'] = df.index.year\n",
    "    \n",
    "    # Calculate technical indicators\n",
    "    df['SMA_50'] = ta.sma(df['Close'], length=50)\n",
    "    df['SMA_200'] = ta.sma(df['Close'], length=200)\n",
    "    df['RSI'] = ta.rsi(df['Close'], length=14)\n",
    "    df['MACD'] = ta.macd(df['Close'])['MACD_12_26_9']\n",
    "    df['MACD_Signal'] = ta.macd(df['Close'])['MACDs_12_26_9']\n",
    "    df['MACD_Hist'] = ta.macd(df['Close'])['MACDh_12_26_9']\n",
    "    \n",
    "    bbands = ta.bbands(df['Close'], length=20, std=2)\n",
    "    df['BB_upper'] = bbands['BBU_20_2.0']\n",
    "    df['BB_middle'] = bbands['BBM_20_2.0']\n",
    "    df['BB_lower'] = bbands['BBL_20_2.0']\n",
    "    \n",
    "    # Create lagged features\n",
    "    df['Lag_1'] = df['Close'].shift(1)\n",
    "    df['Lag_2'] = df['Close'].shift(2)\n",
    "    \n",
    "    # Calculate rolling statistics\n",
    "    df['Rolling_Mean_50'] = df['Close'].rolling(window=50).mean()\n",
    "    df['Rolling_Std_50'] = df['Close'].rolling(window=50).std()\n",
    "    \n",
    "    # Drop rows with NaN values generated from lag and rolling calculations\n",
    "    df.dropna(inplace=True)\n",
    "    \n",
    "    # Check the number of samples available\n",
    "    print(f\"Number of samples after preprocessing: {df.shape[0]}\")\n",
    "    \n",
    "    # Ensure there are enough samples\n",
    "    if df.shape[0] == 0:\n",
    "        raise ValueError(\"No samples available after preprocessing. Adjust the preprocessing steps or check the data.\")\n",
    "    \n",
    "    # Define the target variable and features\n",
    "    target = 'Close'\n",
    "    features = df.columns.drop(target)\n",
    "    \n",
    "    X = df[features]\n",
    "    y = df[target]\n",
    "    \n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "    \n",
    "    # Baseline Model: Linear Regression\n",
    "    lr = LinearRegression()\n",
    "    lr.fit(X_train, y_train)\n",
    "    y_pred_lr = lr.predict(X_test)\n",
    "    print(f\"Linear Regression MSE: {mean_squared_error(y_test, y_pred_lr)}\")\n",
    "    \n",
    "    # Advanced Model: Decision Tree with Grid Search\n",
    "    dt = DecisionTreeRegressor()\n",
    "    dt_params = {'max_depth': [5, 10, 15], 'min_samples_split': [2, 5, 10]}\n",
    "    dt_grid = GridSearchCV(dt, dt_params, cv=5)\n",
    "    dt_grid.fit(X_train, y_train)\n",
    "    y_pred_dt = dt_grid.predict(X_test)\n",
    "    print(f\"Decision Tree MSE: {mean_squared_error(y_test, y_pred_dt)}\")\n",
    "    \n",
    "    # Advanced Model: Random Forest with Random Search\n",
    "    rf = RandomForestRegressor()\n",
    "    rf_params = {'n_estimators': [50, 100, 200], 'max_features': ['auto', 'sqrt', 'log2'], 'max_depth': [5, 10, 15]}\n",
    "    rf_random = RandomizedSearchCV(rf, rf_params, cv=5, n_iter=10)\n",
    "    rf_random.fit(X_train, y_train)\n",
    "    y_pred_rf = rf_random.predict(X_test)\n",
    "    print(f\"Random Forest MSE: {mean_squared_error(y_test, y_pred_rf)}\")\n",
    "    \n",
    "    # Advanced Model: Gradient Boosting with Grid Search\n",
    "    gb = GradientBoostingRegressor()\n",
    "    gb_params = {'n_estimators': [50, 100, 200], 'learning_rate': [0.01, 0.1, 0.2], 'max_depth': [3, 5, 7]}\n",
    "    gb_grid = GridSearchCV(gb, gb_params, cv=5)\n",
    "    gb_grid.fit(X_train, y_train)\n",
    "    y_pred_gb = gb_grid.predict(X_test)\n",
    "    print(f\"Gradient Boosting MSE: {mean_squared_error(y_test, y_pred_gb)}\")\n",
    "    \n",
    "    # Time Series Model: ARIMA\n",
    "    arima = ARIMA(y_train, order=(5, 1, 0))\n",
    "    arima_fit = arima.fit()\n",
    "    y_pred_arima = arima_fit.forecast(steps=len(y_test))\n",
    "    print(f\"ARIMA MSE: {mean_squared_error(y_test, y_pred_arima)}\")\n",
    "    \n",
    "    # Time Series Model: LSTM\n",
    "    n_input = 5\n",
    "    n_features = len(features)\n",
    "    generator = TimeseriesGenerator(X_train, y_train, length=n_input, batch_size=1)\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(LSTM(100, activation='relu', input_shape=(n_input, n_features)))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    model.fit(generator, epochs=50)\n",
    "    \n",
    "    # Preparing test data for LSTM prediction\n",
    "    test_generator = TimeseriesGenerator(X_test, y_test, length=n_input, batch_size=1)\n",
    "    y_pred_lstm = model.predict(test_generator)\n",
    "    y_test_lstm = y_test[n_input:]\n",
    "    print(f\"LSTM MSE: {mean_squared_error(y_test_lstm, y_pred_lstm)}\")\n",
    "    \n",
    "    # Plot time series of stock prices\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    plt.plot(df.index, df['Close'], label='Close Price')\n",
    "    plt.title('AAPL Stock Close Price')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Normalized Price')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Plot technical indicators\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    plt.plot(df.index, df['Close'], label='Close Price')\n",
    "    plt.plot(df.index, df['SMA_50'], label='50-day SMA')\n",
    "    plt.plot(df.index, df['SMA_200'], label='200-day SMA')\n",
    "    plt.title('AAPL Stock Close Price and Moving Averages')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Normalized Price')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    plt.figure(figsize=(14, 7))\n",
    "    plt.plot(df.index, df['MACD'], label='MACD')\n",
    "    plt.plot(df.index, df['MACD_Signal'], label='MACD Signal')\n",
    "    plt.bar(df.index, df['MACD_Hist'], label='MACD Histogram')\n",
    "    plt.title('AAPL MACD')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Value')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    plt.plot(df.index, df['Close'], label='Close Price')\n",
    "    plt.plot(df.index, df['BB_upper'], label='Bollinger Band Upper')\n",
    "    plt.plot(df.index, df['BB_middle'], label='Bollinger Band Middle')\n",
    "    plt.plot(df.index, df['BB_lower'], label='Bollinger Band Lower')\n",
    "    plt.title('AAPL Bollinger Bands')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Normalized Price')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Failed to fetch data from the API.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6d821952-d9d5-4a70-bd23-e26607e3d2fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial number of samples: 100\n",
      "Number of samples after dropping missing values: 100\n",
      "Number of samples after converting to numeric: 100\n",
      "Number of samples after normalization: 100\n",
      "Number of samples after extracting day, month, year: 100\n",
      "Number of samples after calculating technical indicators: 100\n",
      "Number of samples after creating lagged features: 100\n",
      "Number of samples after calculating rolling statistics: 100\n",
      "Number of samples after dropping NaN values from lag and rolling calculations: 0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "With n_samples=0, test_size=0.2 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 118\u001b[0m\n\u001b[0;32m    115\u001b[0m y \u001b[38;5;241m=\u001b[39m df[target]\n\u001b[0;32m    117\u001b[0m \u001b[38;5;66;03m# Split the data into training and testing sets\u001b[39;00m\n\u001b[1;32m--> 118\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_test_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;66;03m# Baseline Model: Linear Regression\u001b[39;00m\n\u001b[0;32m    121\u001b[0m lr \u001b[38;5;241m=\u001b[39m LinearRegression()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n\u001b[1;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    223\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:2660\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[1;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[0;32m   2657\u001b[0m arrays \u001b[38;5;241m=\u001b[39m indexable(\u001b[38;5;241m*\u001b[39marrays)\n\u001b[0;32m   2659\u001b[0m n_samples \u001b[38;5;241m=\u001b[39m _num_samples(arrays[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m-> 2660\u001b[0m n_train, n_test \u001b[38;5;241m=\u001b[39m \u001b[43m_validate_shuffle_split\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2661\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefault_test_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.25\u001b[39;49m\n\u001b[0;32m   2662\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2664\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m shuffle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m   2665\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stratify \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:2308\u001b[0m, in \u001b[0;36m_validate_shuffle_split\u001b[1;34m(n_samples, test_size, train_size, default_test_size)\u001b[0m\n\u001b[0;32m   2305\u001b[0m n_train, n_test \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(n_train), \u001b[38;5;28mint\u001b[39m(n_test)\n\u001b[0;32m   2307\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_train \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 2308\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2309\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWith n_samples=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, test_size=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m and train_size=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2310\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresulting train set will be empty. Adjust any of the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2311\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maforementioned parameters.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(n_samples, test_size, train_size)\n\u001b[0;32m   2312\u001b[0m     )\n\u001b[0;32m   2314\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m n_train, n_test\n",
      "\u001b[1;31mValueError\u001b[0m: With n_samples=0, test_size=0.2 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters."
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas_ta as ta\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n",
    "import numpy as np\n",
    "\n",
    "# Replace with the actual API URL (including endpoint)\n",
    "url = \"https://www.alphavantage.co/query\"\n",
    "\n",
    "# Replace with your actual API key\n",
    "api_key = \"1EB6SE6BZ0SWH6PE\"\n",
    "\n",
    "# Parameters for the API call\n",
    "params = {\n",
    "    \"function\": \"TIME_SERIES_DAILY\",\n",
    "    \"symbol\": \"AAPL\",\n",
    "    \"apikey\": api_key\n",
    "}\n",
    "\n",
    "# Making the request\n",
    "response = requests.get(url, params=params)\n",
    "\n",
    "# Checking if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Parsing the JSON response\n",
    "    data = response.json()\n",
    "    \n",
    "    # Extracting the time series data\n",
    "    time_series = data.get(\"Time Series (Daily)\", {})\n",
    "    \n",
    "    # Converting the time series data to a DataFrame\n",
    "    df = pd.DataFrame.from_dict(time_series, orient='index')\n",
    "    \n",
    "    # Converting index to datetime\n",
    "    df.index = pd.to_datetime(df.index)\n",
    "    \n",
    "    # Sorting the DataFrame by index (date)\n",
    "    df.sort_index(inplace=True)\n",
    "    \n",
    "    # Renaming columns for better readability\n",
    "    df.columns = ['Open', 'High', 'Low', 'Close', 'Volume']\n",
    "    \n",
    "    # Initial number of samples\n",
    "    print(f\"Initial number of samples: {df.shape[0]}\")\n",
    "    \n",
    "    # Handle missing values (remove rows with missing values)\n",
    "    df.dropna(inplace=True)\n",
    "    print(f\"Number of samples after dropping missing values: {df.shape[0]}\")\n",
    "    \n",
    "    # Convert columns to numeric type\n",
    "    df = df.apply(pd.to_numeric)\n",
    "    print(f\"Number of samples after converting to numeric: {df.shape[0]}\")\n",
    "    \n",
    "    # Normalize/standardize numerical features\n",
    "    scaler = StandardScaler()\n",
    "    df[['Open', 'High', 'Low', 'Close', 'Volume']] = scaler.fit_transform(df[['Open', 'High', 'Low', 'Close', 'Volume']])\n",
    "    print(f\"Number of samples after normalization: {df.shape[0]}\")\n",
    "    \n",
    "    # Extracting day, month, year from the index (date)\n",
    "    df['Day'] = df.index.day\n",
    "    df['Month'] = df.index.month\n",
    "    df['Year'] = df.index.year\n",
    "    print(f\"Number of samples after extracting day, month, year: {df.shape[0]}\")\n",
    "    \n",
    "    # Calculate technical indicators\n",
    "    df['SMA_50'] = ta.sma(df['Close'], length=50)\n",
    "    df['SMA_200'] = ta.sma(df['Close'], length=200)\n",
    "    df['RSI'] = ta.rsi(df['Close'], length=14)\n",
    "    df['MACD'] = ta.macd(df['Close'])['MACD_12_26_9']\n",
    "    df['MACD_Signal'] = ta.macd(df['Close'])['MACDs_12_26_9']\n",
    "    df['MACD_Hist'] = ta.macd(df['Close'])['MACDh_12_26_9']\n",
    "    \n",
    "    bbands = ta.bbands(df['Close'], length=20, std=2)\n",
    "    df['BB_upper'] = bbands['BBU_20_2.0']\n",
    "    df['BB_middle'] = bbands['BBM_20_2.0']\n",
    "    df['BB_lower'] = bbands['BBL_20_2.0']\n",
    "    \n",
    "    print(f\"Number of samples after calculating technical indicators: {df.shape[0]}\")\n",
    "    \n",
    "    # Create lagged features\n",
    "    df['Lag_1'] = df['Close'].shift(1)\n",
    "    df['Lag_2'] = df['Close'].shift(2)\n",
    "    print(f\"Number of samples after creating lagged features: {df.shape[0]}\")\n",
    "    \n",
    "    # Calculate rolling statistics\n",
    "    df['Rolling_Mean_50'] = df['Close'].rolling(window=50).mean()\n",
    "    df['Rolling_Std_50'] = df['Close'].rolling(window=50).std()\n",
    "    print(f\"Number of samples after calculating rolling statistics: {df.shape[0]}\")\n",
    "    \n",
    "    # Drop rows with NaN values generated from lag and rolling calculations\n",
    "    df.dropna(inplace=True)\n",
    "    print(f\"Number of samples after dropping NaN values from lag and rolling calculations: {df.shape[0]}\")\n",
    "    \n",
    "    # Check the number of samples available\n",
    "    if df.shape[0] == 100:\n",
    "        raise ValueError(\"No samples available after preprocessing. Adjust the preprocessing steps or check the data.\")\n",
    "    \n",
    "    # Define the target variable and features\n",
    "    target = 'Close'\n",
    "    features = df.columns.drop(target)\n",
    "    \n",
    "    X = df[features]\n",
    "    y = df[target]\n",
    "    \n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "    \n",
    "    # Baseline Model: Linear Regression\n",
    "    lr = LinearRegression()\n",
    "    lr.fit(X_train, y_train)\n",
    "    y_pred_lr = lr.predict(X_test)\n",
    "    print(f\"Linear Regression MSE: {mean_squared_error(y_test, y_pred_lr)}\")\n",
    "    \n",
    "    # Advanced Model: Decision Tree with Grid Search\n",
    "    dt = DecisionTreeRegressor()\n",
    "    dt_params = {'max_depth': [5, 10, 15], 'min_samples_split': [2, 5, 10]}\n",
    "    dt_grid = GridSearchCV(dt, dt_params, cv=5)\n",
    "    dt_grid.fit(X_train, y_train)\n",
    "    y_pred_dt = dt_grid.predict(X_test)\n",
    "    print(f\"Decision Tree MSE: {mean_squared_error(y_test, y_pred_dt)}\")\n",
    "    \n",
    "    # Advanced Model: Random Forest with Random Search\n",
    "    rf = RandomForestRegressor()\n",
    "    rf_params = {'n_estimators': [50, 100, 200], 'max_features': ['auto', 'sqrt', 'log2'], 'max_depth': [5, 10, 15]}\n",
    "    rf_random = RandomizedSearchCV(rf, rf_params, cv=5, n_iter=10)\n",
    "    rf_random.fit(X_train, y_train)\n",
    "    y_pred_rf = rf_random.predict(X_test)\n",
    "    print(f\"Random Forest MSE: {mean_squared_error(y_test, y_pred_rf)}\")\n",
    "    \n",
    "    # Advanced Model: Gradient Boosting with Grid Search\n",
    "    gb = GradientBoostingRegressor()\n",
    "    gb_params = {'n_estimators': [50, 100, 200], 'learning_rate': [0.01, 0.1, 0.2], 'max_depth': [3, 5, 7]}\n",
    "    gb_grid = GridSearchCV(gb, gb_params, cv=5)\n",
    "    gb_grid.fit(X_train, y_train)\n",
    "    y_pred_gb = gb_grid.predict(X_test)\n",
    "    print(f\"Gradient Boosting MSE: {mean_squared_error(y_test, y_pred_gb)}\")\n",
    "    \n",
    "    # Time Series Model: ARIMA\n",
    "    arima = ARIMA(y_train, order=(5, 1, 0))\n",
    "    arima_fit = arima.fit()\n",
    "    y_pred_arima = arima_fit.forecast(steps=len(y_test))\n",
    "    print(f\"ARIMA MSE: {mean_squared_error(y_test, y_pred_arima)}\")\n",
    "    \n",
    "    # Time Series Model: LSTM\n",
    "    n_input = 5\n",
    "    n_features = len(features)\n",
    "    generator = TimeseriesGenerator(X_train, y_train, length=n_input, batch_size=1)\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(LSTM(100, activation='relu', input_shape=(n_input, n_features)))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    model.fit(generator, epochs=50)\n",
    "    \n",
    "    # Preparing test data for LSTM prediction\n",
    "    test_generator = TimeseriesGenerator(X_test, y_test, length=n_input, batch_size=1)\n",
    "    y_pred_lstm = model.predict(test_generator)\n",
    "    y_test_lstm = y_test[n_input:]\n",
    "    print(f\"LSTM MSE: {mean_squared_error(y_test_lstm, y_pred_lstm)}\")\n",
    "    \n",
    "    # Plot time series of stock prices\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    plt.plot(df.index, df['Close'], label='Close Price')\n",
    "    plt.title('AAPL Stock Close Price')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Normalized Price')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Plot technical indicators\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    plt.plot(df.index, df['Close'], label='Close Price')\n",
    "    plt.plot(df.index, df['SMA_50'], label='50-day SMA')\n",
    "    plt.plot(df.index, df['SMA_200'], label='200-day SMA')\n",
    "    plt.title('AAPL Stock Close Price and Moving Averages')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Normalized Price')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    plt.figure(figsize=(14, 7))\n",
    "    plt.plot(df.index, df['MACD'], label='MACD')\n",
    "    plt.plot(df.index, df['MACD_Signal'], label='MACD Signal')\n",
    "    plt.bar(df.index, df['MACD_Hist'], label='MACD Histogram')\n",
    "    plt.title('AAPL MACD')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Value')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    plt.plot(df.index, df['Close'], label='Close Price')\n",
    "    plt.plot(df.index, df['BB_upper'], label='Bollinger Band Upper')\n",
    "    plt.plot(df.index, df['BB_middle'], label='Bollinger Band Middle')\n",
    "    plt.plot(df.index, df['BB_lower'], label='Bollinger Band Lower')\n",
    "    plt.title('AAPL Bollinger Bands')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Normalized Price')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Failed to fetch data from the API.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eb62f3d9-324e-4fd5-805b-8215712417ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial number of samples: 100\n",
      "Number of samples after dropping missing values: 100\n",
      "Number of samples after converting to numeric: 100\n",
      "Number of samples after normalization: 100\n",
      "Number of samples after extracting day, month, year: 100\n",
      "Number of samples after calculating technical indicators: 100\n",
      "Number of samples after creating lagged features: 100\n",
      "Number of samples after calculating rolling statistics: 100\n",
      "Number of samples after dropping NaN values from lag and rolling calculations: 0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "With n_samples=0, test_size=0.2 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 118\u001b[0m\n\u001b[0;32m    115\u001b[0m y \u001b[38;5;241m=\u001b[39m df[target]\n\u001b[0;32m    117\u001b[0m \u001b[38;5;66;03m# Split the data into training and testing sets\u001b[39;00m\n\u001b[1;32m--> 118\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_test_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;66;03m# Baseline Model: Linear Regression\u001b[39;00m\n\u001b[0;32m    121\u001b[0m lr \u001b[38;5;241m=\u001b[39m LinearRegression()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n\u001b[1;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    223\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:2660\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[1;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[0;32m   2657\u001b[0m arrays \u001b[38;5;241m=\u001b[39m indexable(\u001b[38;5;241m*\u001b[39marrays)\n\u001b[0;32m   2659\u001b[0m n_samples \u001b[38;5;241m=\u001b[39m _num_samples(arrays[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m-> 2660\u001b[0m n_train, n_test \u001b[38;5;241m=\u001b[39m \u001b[43m_validate_shuffle_split\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2661\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefault_test_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.25\u001b[39;49m\n\u001b[0;32m   2662\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2664\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m shuffle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m   2665\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stratify \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:2308\u001b[0m, in \u001b[0;36m_validate_shuffle_split\u001b[1;34m(n_samples, test_size, train_size, default_test_size)\u001b[0m\n\u001b[0;32m   2305\u001b[0m n_train, n_test \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(n_train), \u001b[38;5;28mint\u001b[39m(n_test)\n\u001b[0;32m   2307\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_train \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 2308\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2309\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWith n_samples=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, test_size=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m and train_size=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2310\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresulting train set will be empty. Adjust any of the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2311\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maforementioned parameters.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(n_samples, test_size, train_size)\n\u001b[0;32m   2312\u001b[0m     )\n\u001b[0;32m   2314\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m n_train, n_test\n",
      "\u001b[1;31mValueError\u001b[0m: With n_samples=0, test_size=0.2 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters."
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas_ta as ta\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n",
    "import numpy as np\n",
    "\n",
    "# Replace with the actual API URL (including endpoint)\n",
    "url = \"https://www.alphavantage.co/query\"\n",
    "\n",
    "# Replace with your actual API key\n",
    "api_key = \"1EB6SE6BZ0SWH6PE\"\n",
    "\n",
    "# Parameters for the API call\n",
    "params = {\n",
    "    \"function\": \"TIME_SERIES_DAILY\",\n",
    "    \"symbol\": \"AAPL\",\n",
    "    \"apikey\": api_key\n",
    "}\n",
    "\n",
    "# Making the request\n",
    "response = requests.get(url, params=params)\n",
    "\n",
    "# Checking if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Parsing the JSON response\n",
    "    data = response.json()\n",
    "    \n",
    "    # Extracting the time series data\n",
    "    time_series = data.get(\"Time Series (Daily)\", {})\n",
    "    \n",
    "    # Converting the time series data to a DataFrame\n",
    "    df = pd.DataFrame.from_dict(time_series, orient='index')\n",
    "    \n",
    "    # Converting index to datetime\n",
    "    df.index = pd.to_datetime(df.index)\n",
    "    \n",
    "    # Sorting the DataFrame by index (date)\n",
    "    df.sort_index(inplace=True)\n",
    "    \n",
    "    # Renaming columns for better readability\n",
    "    df.columns = ['Open', 'High', 'Low', 'Close', 'Volume']\n",
    "    \n",
    "    # Initial number of samples\n",
    "    print(f\"Initial number of samples: {df.shape[0]}\")\n",
    "    \n",
    "    # Handle missing values (remove rows with missing values)\n",
    "    df.dropna(inplace=True)\n",
    "    print(f\"Number of samples after dropping missing values: {df.shape[0]}\")\n",
    "    \n",
    "    # Convert columns to numeric type\n",
    "    df = df.apply(pd.to_numeric)\n",
    "    print(f\"Number of samples after converting to numeric: {df.shape[0]}\")\n",
    "    \n",
    "    # Normalize/standardize numerical features\n",
    "    scaler = StandardScaler()\n",
    "    df[['Open', 'High', 'Low', 'Close', 'Volume']] = scaler.fit_transform(df[['Open', 'High', 'Low', 'Close', 'Volume']])\n",
    "    print(f\"Number of samples after normalization: {df.shape[0]}\")\n",
    "    \n",
    "    # Extracting day, month, year from the index (date)\n",
    "    df['Day'] = df.index.day\n",
    "    df['Month'] = df.index.month\n",
    "    df['Year'] = df.index.year\n",
    "    print(f\"Number of samples after extracting day, month, year: {df.shape[0]}\")\n",
    "    \n",
    "    # Calculate technical indicators\n",
    "    df['SMA_50'] = ta.sma(df['Close'], length=50)\n",
    "    df['SMA_200'] = ta.sma(df['Close'], length=200)\n",
    "    df['RSI'] = ta.rsi(df['Close'], length=14)\n",
    "    df['MACD'] = ta.macd(df['Close'])['MACD_12_26_9']\n",
    "    df['MACD_Signal'] = ta.macd(df['Close'])['MACDs_12_26_9']\n",
    "    df['MACD_Hist'] = ta.macd(df['Close'])['MACDh_12_26_9']\n",
    "    \n",
    "    bbands = ta.bbands(df['Close'], length=20, std=2)\n",
    "    df['BB_upper'] = bbands['BBU_20_2.0']\n",
    "    df['BB_middle'] = bbands['BBM_20_2.0']\n",
    "    df['BB_lower'] = bbands['BBL_20_2.0']\n",
    "    \n",
    "    print(f\"Number of samples after calculating technical indicators: {df.shape[0]}\")\n",
    "    \n",
    "    # Create lagged features\n",
    "    df['Lag_1'] = df['Close'].shift(1)\n",
    "    df['Lag_2'] = df['Close'].shift(2)\n",
    "    print(f\"Number of samples after creating lagged features: {df.shape[0]}\")\n",
    "    \n",
    "    # Calculate rolling statistics\n",
    "    df['Rolling_Mean_50'] = df['Close'].rolling(window=50).mean()\n",
    "    df['Rolling_Std_50'] = df['Close'].rolling(window=50).std()\n",
    "    print(f\"Number of samples after calculating rolling statistics: {df.shape[0]}\")\n",
    "    \n",
    "    # Drop rows with NaN values generated from lag and rolling calculations\n",
    "    df.dropna(inplace=True)\n",
    "    print(f\"Number of samples after dropping NaN values from lag and rolling calculations: {df.shape[0]}\")\n",
    "    \n",
    "    # Check the number of samples available\n",
    "    if df.shape[0] == 15:\n",
    "        raise ValueError(\"No samples available after preprocessing. Adjust the preprocessing steps or check the data.\")\n",
    "    \n",
    "    # Define the target variable and features\n",
    "    target = 'Close'\n",
    "    features = df.columns.drop(target)\n",
    "    \n",
    "    X = df[features]\n",
    "    y = df[target]\n",
    "    \n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "    \n",
    "    # Baseline Model: Linear Regression\n",
    "    lr = LinearRegression()\n",
    "    lr.fit(X_train, y_train)\n",
    "    y_pred_lr = lr.predict(X_test)\n",
    "    print(f\"Linear Regression MSE: {mean_squared_error(y_test, y_pred_lr)}\")\n",
    "    \n",
    "    # Advanced Model: Decision Tree with Grid Search\n",
    "    dt = DecisionTreeRegressor()\n",
    "    dt_params = {'max_depth': [5, 10, 15], 'min_samples_split': [2, 5, 10]}\n",
    "    dt_grid = GridSearchCV(dt, dt_params, cv=5)\n",
    "    dt_grid.fit(X_train, y_train)\n",
    "    y_pred_dt = dt_grid.predict(X_test)\n",
    "    print(f\"Decision Tree MSE: {mean_squared_error(y_test, y_pred_dt)}\")\n",
    "    \n",
    "    # Advanced Model: Random Forest with Random Search\n",
    "    rf = RandomForestRegressor()\n",
    "    rf_params = {'n_estimators': [50, 100, 200], 'max_features': ['auto', 'sqrt', 'log2'], 'max_depth': [5, 10, 15]}\n",
    "    rf_random = RandomizedSearchCV(rf, rf_params, cv=5, n_iter=10)\n",
    "    rf_random.fit(X_train, y_train)\n",
    "    y_pred_rf = rf_random.predict(X_test)\n",
    "    print(f\"Random Forest MSE: {mean_squared_error(y_test, y_pred_rf)}\")\n",
    "    \n",
    "    # Advanced Model: Gradient Boosting with Grid Search\n",
    "    gb = GradientBoostingRegressor()\n",
    "    gb_params = {'n_estimators': [50, 100, 200], 'learning_rate': [0.01, 0.1, 0.2], 'max_depth': [3, 5, 7]}\n",
    "    gb_grid = GridSearchCV(gb, gb_params, cv=5)\n",
    "    gb_grid.fit(X_train, y_train)\n",
    "    y_pred_gb = gb_grid.predict(X_test)\n",
    "    print(f\"Gradient Boosting MSE: {mean_squared_error(y_test, y_pred_gb)}\")\n",
    "    \n",
    "    # Time Series Model: ARIMA\n",
    "    arima = ARIMA(y_train, order=(5, 1, 0))\n",
    "    arima_fit = arima.fit()\n",
    "    y_pred_arima = arima_fit.forecast(steps=len(y_test))\n",
    "    print(f\"ARIMA MSE: {mean_squared_error(y_test, y_pred_arima)}\")\n",
    "    \n",
    "    # Time Series Model: LSTM\n",
    "    n_input = 5\n",
    "    n_features = len(features)\n",
    "    generator = TimeseriesGenerator(X_train, y_train, length=n_input, batch_size=1)\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(LSTM(100, activation='relu', input_shape=(n_input, n_features)))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    model.fit(generator, epochs=50)\n",
    "    \n",
    "    # Preparing test data for LSTM prediction\n",
    "    test_generator = TimeseriesGenerator(X_test, y_test, length=n_input, batch_size=1)\n",
    "    y_pred_lstm = model.predict(test_generator)\n",
    "    y_test_lstm = y_test[n_input:]\n",
    "    print(f\"LSTM MSE: {mean_squared_error(y_test_lstm, y_pred_lstm)}\")\n",
    "    \n",
    "    # Plot time series of stock prices\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    plt.plot(df.index, df['Close'], label='Close Price')\n",
    "    plt.title('AAPL Stock Close Price')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Normalized Price')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Plot technical indicators\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    plt.plot(df.index, df['Close'], label='Close Price')\n",
    "    plt.plot(df.index, df['SMA_50'], label='50-day SMA')\n",
    "    plt.plot(df.index, df['SMA_200'], label='200-day SMA')\n",
    "    plt.title('AAPL Stock Close Price and Moving Averages')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Normalized Price')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    plt.figure(figsize=(14, 7))\n",
    "    plt.plot(df.index, df['MACD'], label='MACD')\n",
    "    plt.plot(df.index, df['MACD_Signal'], label='MACD Signal')\n",
    "    plt.bar(df.index, df['MACD_Hist'], label='MACD Histogram')\n",
    "    plt.title('AAPL MACD')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Value')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    plt.plot(df.index, df['Close'], label='Close Price')\n",
    "    plt.plot(df.index, df['BB_upper'], label='Bollinger Band Upper')\n",
    "    plt.plot(df.index, df['BB_middle'], label='Bollinger Band Middle')\n",
    "    plt.plot(df.index, df['BB_lower'], label='Bollinger Band Lower')\n",
    "    plt.title('AAPL Bollinger Bands')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Normalized Price')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Failed to fetch data from the API.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1207a6e9-c2d0-4499-9968-cef93103ccab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
